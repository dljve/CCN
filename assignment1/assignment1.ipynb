{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Computational Cognitive Neuroscience\n",
    "\n",
    "Douwe van Erp (s4258126) & Arianne Meijer - van de Griend (s4620135)\n",
    "\n",
    "# Assignment 1 - Training an MLP on MNIST\n",
    " \n",
    "The goal of the practical assignments is to become proficient at the development of SOTA neural network models. In this first assignment, you will learn to implement a multilayer perceptron in [Chainer](https://chainer.org). If you have little prior experience with neural networks, you should acquaint yourself with the basics. To this end you should read at least Chapters 1 and 2 on the following [website](http://neuralnetworksanddeeplearning.com).\n",
    " \n",
    "To prepare yourself, you should study the [Chainer tutorial](https://docs.chainer.org/en/stable/tutorial/index.html). To really understand the framework, you are required to use low-level Chainer code. This means that you cannot use the Trainer and Updater objects (that is, you need to implement the training loops yourself). Note also that it is easiest to implement your own data iterators. We provide an iterator which creates batches of data in utils.py. It can be convenient and is allowed to use the Classifier object.\n",
    "\n",
    "## Instructions\n",
    "Train a multilayer perceptron which has one hidden layer consisting of 10 hidden units on the MNIST dataset. The goal is to classify handwritten characters. To speed up training, use get_mnist in utils.py with n_train=n_test=100. Train the network for 20 training epochs. Use a batch size of 32. Plot the decrease in training and test loss over epochs. Submit your code as a documented Jupyter notebook, which includes your figure. For development, it is advised to first implement your work in PyCharm and then port your code to a notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import Chain\n",
    "from chainer import iterators, optimizers\n",
    "from chainer import report, training\n",
    "from chainer.training import extensions\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of the multilayer perceptron class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP(Chain):\n",
    "    def __init__(self, n_units, n_out):\n",
    "        super(MLP, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(None, n_units)\n",
    "            self.l2 = L.Linear(None, n_out)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        y = self.l2(h1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of a Classifier with softmax cross entropy as loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Classifier(Chain):\n",
    "    def __init__(self,predictor):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.predictor = predictor\n",
    "        \n",
    "    def __call__(self, x, t):\n",
    "        y = self.predictor(x)\n",
    "        loss = F.softmax_cross_entropy(y, t)\n",
    "        accuracy = F.accuracy(y, t)\n",
    "        report({'loss': loss, 'accuracy' : accuracy}, self)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the train and test data and define the iterators for those datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = utils.get_mnist(n_train=100, n_test=100, n_dim=1, with_label=True)\n",
    "\n",
    "# Batch size 32\n",
    "train_iter = iterators.SerialIterator(train, batch_size=32, shuffle=True)\n",
    "test_iter = iterators.SerialIterator(test, batch_size=32, repeat=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain the different components of the model.\n",
    "Define the model, optimizer, updater and trainer.\n",
    "Also add the visualizations of the progress to the trainer.\n",
    "Then run the trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     total [########..........................................] 16.00%\n",
      "this epoch [##########........................................] 20.00%\n",
      "       100 iter, 3 epoch / 20 epochs\n",
      "       inf iters/sec. Estimated time to finish: 0:00:00.\n",
      "     total [################..................................] 32.00%\n",
      "this epoch [####################..............................] 40.00%\n",
      "       200 iter, 6 epoch / 20 epochs\n",
      "     122.1 iters/sec. Estimated time to finish: 0:00:03.480750.\n",
      "     total [########################..........................] 48.00%\n",
      "this epoch [#############################.....................] 60.00%\n",
      "       300 iter, 9 epoch / 20 epochs\n",
      "    123.92 iters/sec. Estimated time to finish: 0:00:02.622750.\n",
      "     total [################################..................] 64.00%\n",
      "this epoch [########################################..........] 80.00%\n",
      "       400 iter, 12 epoch / 20 epochs\n",
      "    126.85 iters/sec. Estimated time to finish: 0:00:01.773750.\n",
      "     total [########################################..........] 80.00%\n",
      "this epoch [..................................................]  0.00%\n",
      "       500 iter, 16 epoch / 20 epochs\n",
      "    116.65 iters/sec. Estimated time to finish: 0:00:01.071563.\n",
      "     total [################################################..] 96.00%\n",
      "this epoch [#########.........................................] 20.00%\n",
      "       600 iter, 19 epoch / 20 epochs\n",
      "    117.34 iters/sec. Estimated time to finish: 0:00:00.213050.\n"
     ]
    }
   ],
   "source": [
    "model = L.Classifier(MLP(10, 10))  # the input size, 784, is inferred\n",
    "optimizer = optimizers.SGD()\n",
    "optimizer.setup(model)\n",
    "\n",
    "updater = training.StandardUpdater(train_iter, optimizer)\n",
    "trainer = training.Trainer(updater, (20, 'epoch'), out='result')\n",
    "trainer.extend(extensions.Evaluator(test_iter, model))\n",
    "trainer.extend(extensions.LogReport())\n",
    "#trainer.extend(extensions.PlotReport(['main/accuracy'],'iteration'))\n",
    "#trainer.extend(extensions.PrintReport(['epoch', 'main/accuracy', 'validation/main/accuracy']))\n",
    "trainer.extend(extensions.PlotReport(['main/loss', 'validation/main/loss'], 'epoch'))\n",
    "trainer.extend(extensions.ProgressBar())\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](result/plot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
