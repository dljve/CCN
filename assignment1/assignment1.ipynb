{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Computational Cognitive Neuroscience\n",
    "\n",
    "## Assignment 1 - Training an MLP on MNIST\n",
    "\n",
    "#### Douwe van Erp (s4258126) & Arianne Meijer - van de Griend (s4620135)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import Chain\n",
    "from chainer import iterators, optimizers\n",
    "from chainer import report, training\n",
    "from chainer.training import extensions\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *MLP* class specificies the architecture of the multilayer perceptron. It consists of two layers: one hidden layer with *n_units* hidden units and one output layer with *n_out* output units. We use the *rectified linear unit* (ReLu) as  activation function for the hidden layer, since it is effective for image recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP(Chain):\n",
    "    def __init__(self, n_units, n_out):\n",
    "        super(MLP, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(None, n_units)\n",
    "            self.l2 = L.Linear(None, n_out)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        y = self.l2(h1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Classifier* class on top of the *MLP* class specifies the *softmax* function as the classification function, and the *cross entropy loss* as loss function. By evoking the *report* command, Chainer will report the accuracy and loss while training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Classifier(Chain):\n",
    "    def __init__(self,predictor):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.predictor = predictor\n",
    "        \n",
    "    def __call__(self, x, t):\n",
    "        y = self.predictor(x)\n",
    "        loss = F.softmax_cross_entropy(y, t)\n",
    "        accuracy = F.accuracy(y, t)\n",
    "        report({'loss': loss, 'accuracy' : accuracy}, self)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can chain the different components of the model. We initialize our multilayer perceptron with 10 hidden units and 10 output units. Additionally the softmax classifier is added to the MLP. Finally we use stochastic gradient descent (SGD) to optimize our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = L.Classifier(MLP(10, 10))\n",
    "optimizer = optimizers.SGD()\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *get_mnist* function from the *utils* class will select n_train=n_test=100 training and test samples per class. Because there are 10 digits (classes), we obtain a training and test dataset of both 1000 samples and 784 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = utils.get_mnist(n_train=100, n_test=100, n_dim=1, with_label=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chainer's *SerialIterator* is used to create minibatches of size 32. It is also used to shuffle the training data, to avoid the risk of using correlated minibatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iter = iterators.SerialIterator(train, batch_size=32, shuffle=True)\n",
    "test_iter = iterators.SerialIterator(test, batch_size=32, repeat=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model for 20 epochs, while using several extension to report the accuracy and loss during the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       main/accuracy  validation/main/accuracy\n",
      "1           0.193359       0.192383                  \n",
      "2           0.251008       0.25293                   \n",
      "3           0.335685       0.300781                  \n",
      "4           0.396169       0.390625                  \n",
      "5           0.472656       0.47168                   \n",
      "6           0.548387       0.532227                  \n",
      "7           0.597782       0.582031                  \n",
      "8           0.673387       0.615234                  \n",
      "9           0.723633       0.633789                  \n",
      "10          0.751008       0.65332                   \n",
      "11          0.767137       0.678711                  \n",
      "12          0.777218       0.681641                  \n",
      "13          0.796875       0.689453                  \n",
      "14          0.794355       0.706055                  \n",
      "15          0.8125         0.697266                  \n",
      "16          0.816532       0.710938                  \n",
      "17          0.822266       0.729492                  \n",
      "18          0.826613       0.728516                  \n",
      "19          0.829637       0.730469                  \n",
      "20          0.839718       0.737305                  \n"
     ]
    }
   ],
   "source": [
    "updater = training.StandardUpdater(train_iter, optimizer)\n",
    "trainer = training.Trainer(updater, (20, 'epoch'), out='result')\n",
    "trainer.extend(extensions.Evaluator(test_iter, model))\n",
    "trainer.extend(extensions.LogReport())\n",
    "trainer.extend(extensions.PrintReport(['epoch', 'main/accuracy', 'validation/main/accuracy']))\n",
    "trainer.extend(extensions.PlotReport(['main/loss', 'validation/main/loss'], 'epoch'))\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The model achieves an accuracy of 73.7% on the test set. However given the small dataset sizes, the accuracy is highly dependent on which samples are chosen, how they are shuffled, and the random weight initialization of the MLP. The plot shows that the validation loss keeps declining with the training loss, which indicates that the model has not overfitted to the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](result/plot.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
